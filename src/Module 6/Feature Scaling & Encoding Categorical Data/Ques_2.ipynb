{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titanic.csv not found. Please download it from Kaggle or another source.\n",
      "Original DataFrame:\n",
      "   PassengerId  Survived  Pclass Name     Sex   Age  SibSp  Parch Ticket  \\\n",
      "0            1         0       3    A    male  22.0      1      0      A   \n",
      "1            2         1       1    B  female  38.0      1      0      B   \n",
      "2            3         1       3    C  female  26.0      0      0      C   \n",
      "3            4         1       1    D  female  35.0      1      0      D   \n",
      "4            5         0       3    E    male  35.0      0      0      E   \n",
      "\n",
      "    Fare Cabin Embarked  \n",
      "0   7.25   NaN        S  \n",
      "1  71.28   C85        C  \n",
      "2   7.92   NaN        S  \n",
      "3  53.10  C123        S  \n",
      "4   8.05   NaN        S  \n",
      "------------------------------\n",
      "Question 5: Label Encoding vs One-Hot Encoding for 'Sex'\n",
      "\n",
      "DataFrame with Label Encoded 'Sex':\n",
      "      Sex  Sex_LabelEncoded\n",
      "0    male                 1\n",
      "1  female                 0\n",
      "2  female                 0\n",
      "3  female                 0\n",
      "4    male                 1\n",
      "\n",
      "DataFrame with One-Hot Encoded 'Sex':\n",
      "   Sex_OneHot_male\n",
      "0             True\n",
      "1            False\n",
      "2            False\n",
      "3            False\n",
      "4             True\n",
      "------------------------------\n",
      "Question 6: Combining Feature Scaling Techniques\n",
      "\n",
      "Original Fare vs Min-Max Scaled Fare:\n",
      "    Fare  Fare_MinMaxScaled\n",
      "0   7.25           0.000000\n",
      "1  71.28           1.000000\n",
      "2   7.92           0.010464\n",
      "3  53.10           0.716071\n",
      "4   8.05           0.012494\n",
      "\n",
      "Original Fare vs Standard Scaled Fare:\n",
      "    Fare  Fare_StandardScaled\n",
      "0   7.25            -0.832330\n",
      "1  71.28             1.997495\n",
      "2   7.92            -0.802719\n",
      "3  53.10             1.194024\n",
      "4   8.05            -0.796974\n",
      "\n",
      "Explanation: Min-Max scaling scales data to a fixed range (usually [0, 1]), while Standardization scales data to have a mean of 0 and a standard deviation of 1. The outputs show the different distributions of the scaled values compared to the original.\n",
      "------------------------------\n",
      "Question 7: Handling Multiple Categorical Features ('Sex', 'Embarked')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'get_feature_names_in'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 151\u001b[0m\n\u001b[1;32m    145\u001b[0m df_encoded_multiple \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mfit_transform(df)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Get the feature names after encoding\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# This requires accessing the encoder within the ColumnTransformer\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# For sparse_output=False, it's easier to convert back to DataFrame for readability\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Create a list of new column names\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m onehot_feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_transformers_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monehot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_in\u001b[49m(categorical_cols)\n\u001b[1;32m    152\u001b[0m new_column_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(onehot_feature_names) \u001b[38;5;241m+\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m categorical_cols]\n\u001b[1;32m    154\u001b[0m df_encoded_multiple \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df_encoded_multiple, columns\u001b[38;5;241m=\u001b[39mnew_column_names, index\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'get_feature_names_in'"
     ]
    }
   ],
   "source": [
    "# Question 5: Label Encoding vs One-Hot Encoding\n",
    "# Task: Show the difference between Label Encoding and One-Hot Encoding on the Titanic dataset for the 'Sex' feature.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 6: Combining Feature Scaling Techniques\n",
    "# Task: Demonstrate combining Min-Max Scaling and Standardization for the same datasetand explain the results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 7: Handling Multiple Categorical Features\n",
    "# Task: Handle multiple categorical features ('Sex', 'Embarked') from the Titanic dataset using One-Hot Encoding.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 8: Ordinal Encoding for Ranked Categories\n",
    "# Task: Ordinal encode 'Pclass' (Passenger class) from the Titanic dataset considering passenger class as a ranked feature.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Question 9: Impact of Scaling on Different Algorithms\n",
    "# Task: Investigate the impact of different scaling techniques on a decision tree model and compare it with a SVM.\n",
    "\n",
    "\n",
    "\n",
    "# Question 10: Custom Transformations for Categorical Features\n",
    "# Task: Implement a custom transformation function for encoding high cardinality categorical features efficiently.\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('titanic.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"titanic.csv not found. Please download it from Kaggle or another source.\")\n",
    "    # Using a sample dataset structure if titanic.csv is not available for demonstration\n",
    "    data = {'PassengerId': range(1, 11),\n",
    "            'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
    "            'Pclass': [3, 1, 3, 1, 3, 3, 1, 3, 2, 3],\n",
    "            'Name': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "            'Sex': ['male', 'female', 'female', 'female', 'male', 'male', 'male', 'male', 'female', 'male'],\n",
    "            'Age': [22, 38, 26, 35, 35, np.nan, 54, 2, 27, 14],\n",
    "            'SibSp': [1, 1, 0, 1, 0, 0, 0, 3, 1, 0],\n",
    "            'Parch': [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "            'Ticket': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "            'Fare': [7.25, 71.28, 7.92, 53.1, 8.05, 8.45, 51.86, 21.07, 24.0, 7.85],\n",
    "            'Cabin': [np.nan, 'C85', np.nan, 'C123', np.nan, np.nan, 'E46', np.nan, np.nan, np.nan],\n",
    "            'Embarked': ['S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S']}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Question 5: Label Encoding vs One-Hot Encoding for 'Sex'\n",
    "print(\"Question 5: Label Encoding vs One-Hot Encoding for 'Sex'\")\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['Sex_LabelEncoded'] = label_encoder.fit_transform(df['Sex'])\n",
    "print(\"\\nDataFrame with Label Encoded 'Sex':\")\n",
    "print(df[['Sex', 'Sex_LabelEncoded']].head())\n",
    "\n",
    "# One-Hot Encoding (using pandas get_dummies)\n",
    "df_onehot = pd.get_dummies(df, columns=['Sex'], prefix='Sex_OneHot', drop_first=True) # drop_first=True to avoid multicollinearity\n",
    "print(\"\\nDataFrame with One-Hot Encoded 'Sex':\")\n",
    "print(df_onehot[['Sex_OneHot_male']].head()) # Note: 'female' would be 0 when 'male' is 1 and vice versa\n",
    "\n",
    "# Clean up added columns for the next questions\n",
    "df = df.drop(columns=['Sex_LabelEncoded'])\n",
    "df_onehot = df_onehot.drop(columns=['Sex_OneHot_male']) # Revert one-hot for subsequent steps\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Question 6: Combining Feature Scaling Techniques\n",
    "print(\"Question 6: Combining Feature Scaling Techniques\")\n",
    "\n",
    "# Use the 'Fare' column for demonstration\n",
    "# Handle potential missing values in 'Fare' if any (though 'Fare' usually has very few)\n",
    "df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "\n",
    "# Reshape for scikit-learn scalers\n",
    "fare = df[['Fare']]\n",
    "\n",
    "# Min-Max Scaling\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df['Fare_MinMaxScaled'] = minmax_scaler.fit_transform(fare)\n",
    "print(\"\\nOriginal Fare vs Min-Max Scaled Fare:\")\n",
    "print(df[['Fare', 'Fare_MinMaxScaled']].head())\n",
    "\n",
    "# Standardization (Z-score scaling)\n",
    "standard_scaler = StandardScaler()\n",
    "df['Fare_StandardScaled'] = standard_scaler.fit_transform(fare)\n",
    "print(\"\\nOriginal Fare vs Standard Scaled Fare:\")\n",
    "print(df[['Fare', 'Fare_StandardScaled']].head())\n",
    "\n",
    "print(\"\\nExplanation: Min-Max scaling scales data to a fixed range (usually [0, 1]), while Standardization scales data to have a mean of 0 and a standard deviation of 1. The outputs show the different distributions of the scaled values compared to the original.\")\n",
    "\n",
    "# Clean up added columns\n",
    "df = df.drop(columns=['Fare_MinMaxScaled', 'Fare_StandardScaled'])\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Question 7: Handling Multiple Categorical Features\n",
    "print(\"Question 7: Handling Multiple Categorical Features ('Sex', 'Embarked')\")\n",
    "\n",
    "# Handle missing values in 'Embarked'\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "\n",
    "# Select categorical columns\n",
    "categorical_cols = ['Sex', 'Embarked']\n",
    "\n",
    "# Use ColumnTransformer for One-Hot Encoding multiple columns\n",
    "# remainder='passthrough' keeps other columns\n",
    "# sparse_output=False returns a dense numpy array\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[('onehot', onehot_encoder, categorical_cols)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "df_encoded_multiple = ct.fit_transform(df)\n",
    "\n",
    "# Get the feature names after encoding\n",
    "# This requires accessing the encoder within the ColumnTransformer\n",
    "# For sparse_output=False, it's easier to convert back to DataFrame for readability\n",
    "# Create a list of new column names\n",
    "onehot_feature_names = ct.named_transformers_['onehot'].get_feature_names_in(categorical_cols)\n",
    "new_column_names = list(onehot_feature_names) + [col for col in df.columns if col not in categorical_cols]\n",
    "\n",
    "df_encoded_multiple = pd.DataFrame(df_encoded_multiple, columns=new_column_names, index=df.index)\n",
    "\n",
    "print(\"\\nDataFrame with multiple categorical features ('Sex', 'Embarked') One-Hot Encoded:\")\n",
    "# Display relevant columns\n",
    "encoded_cols_display = [col for col in df_encoded_multiple.columns if col.startswith('Sex_') or col.startswith('Embarked_')]\n",
    "print(df_encoded_multiple[encoded_cols_display].head())\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Question 8: Ordinal Encoding for Ranked Categories ('Pclass')\n",
    "print(\"Question 8: Ordinal Encoding for Ranked Categories ('Pclass')\")\n",
    "\n",
    "# 'Pclass' is 1st, 2nd, 3rd. We can assume 1st > 2nd > 3rd.\n",
    "# Define the order of categories explicitly\n",
    "pclass_order = [[1, 2, 3]] # Pclass values as a list within a list for the encoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(categories=pclass_order)\n",
    "\n",
    "# Reshape for scikit-learn encoder\n",
    "pclass = df[['Pclass']]\n",
    "\n",
    "df['Pclass_OrdinalEncoded'] = ordinal_encoder.fit_transform(pclass)\n",
    "\n",
    "print(\"\\nOriginal Pclass vs Ordinal Encoded Pclass:\")\n",
    "print(df[['Pclass', 'Pclass_OrdinalEncoded']].head())\n",
    "\n",
    "print(\"\\nExplanation: Pclass values (1, 2, 3) are encoded preserving their rank, where 1 < 2 < 3 in the encoded output.\")\n",
    "\n",
    "# Clean up added column\n",
    "df = df.drop(columns=['Pclass_OrdinalEncoded'])\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Question 9: Impact of Scaling on Different Algorithms\n",
    "print(\"Question 9: Impact of Scaling on Different Algorithms (Decision Tree vs SVM)\")\n",
    "\n",
    "# Select features and target\n",
    "# Using numerical features: 'Age', 'Fare', 'SibSp', 'Parch'\n",
    "# Using categorical features: 'Sex', 'Embarked', 'Pclass'\n",
    "# Target: 'Survived'\n",
    "\n",
    "features = ['Age', 'Fare', 'SibSp', 'Parch', 'Sex', 'Embarked', 'Pclass']\n",
    "target = 'Survived'\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "# Handle missing values\n",
    "X['Age'] = X['Age'].fillna(X['Age'].median())\n",
    "X['Fare'] = X['Fare'].fillna(X['Fare'].median())\n",
    "X['Embarked'] = X['Embarked'].fillna(X['Embarked'].mode()[0]) # Already done, but good practice\n",
    "\n",
    "# Define preprocessing steps for numerical and categorical features\n",
    "numerical_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
    "categorical_features = ['Sex', 'Embarked', 'Pclass']\n",
    "\n",
    "# Create preprocessors\n",
    "# Preprocessor for unscaled numerical data\n",
    "preprocessor_unscaled = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features), # Keep numerical data as is\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Preprocessor for scaled numerical data\n",
    "preprocessor_scaled = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features), # Scale numerical data\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Train models ---\n",
    "\n",
    "# Decision Tree on Unscaled Data\n",
    "pipeline_dt_unscaled = Pipeline(steps=[('preprocessor', preprocessor_unscaled),\n",
    "                                       ('classifier', DecisionTreeClassifier(random_state=42))])\n",
    "\n",
    "pipeline_dt_unscaled.fit(X_train, y_train)\n",
    "y_pred_dt_unscaled = pipeline_dt_unscaled.predict(X_test)\n",
    "accuracy_dt_unscaled = accuracy_score(y_test, y_pred_dt_unscaled)\n",
    "print(f\"\\nAccuracy (Decision Tree, Unscaled Numerical Features): {accuracy_dt_unscaled:.4f}\")\n",
    "\n",
    "# Decision Tree on Scaled Data\n",
    "pipeline_dt_scaled = Pipeline(steps=[('preprocessor', preprocessor_scaled),\n",
    "                                     ('classifier', DecisionTreeClassifier(random_state=42))])\n",
    "\n",
    "pipeline_dt_scaled.fit(X_train, y_train)\n",
    "y_pred_dt_scaled = pipeline_dt_scaled.predict(X_test)\n",
    "accuracy_dt_scaled = accuracy_score(y_test, y_pred_dt_scaled)\n",
    "print(f\"Accuracy (Decision Tree, Scaled Numerical Features): {accuracy_dt_scaled:.4f}\")\n",
    "\n",
    "# SVM on Unscaled Data\n",
    "# SVM is sensitive to the scale of features, so expect poor performance without scaling\n",
    "# Use probability=True for predict_proba if needed, but not required for basic accuracy\n",
    "pipeline_svm_unscaled = Pipeline(steps=[('preprocessor', preprocessor_unscaled),\n",
    "                                        ('classifier', SVC(gamma='auto', random_state=42))])\n",
    "\n",
    "# Note: SVM can take longer to train\n",
    "try:\n",
    "    pipeline_svm_unscaled.fit(X_train, y_train)\n",
    "    y_pred_svm_unscaled = pipeline_svm_unscaled.predict(X_test)\n",
    "    accuracy_svm_unscaled = accuracy_score(y_test, y_pred_svm_unscaled)\n",
    "    print(f\"Accuracy (SVM, Unscaled Numerical Features): {accuracy_svm_unscaled:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not train SVM on unscaled data (may be due to large feature values): {e}\")\n",
    "    accuracy_svm_unscaled = 'N/A'\n",
    "\n",
    "\n",
    "# SVM on Scaled Data\n",
    "pipeline_svm_scaled = Pipeline(steps=[('preprocessor', preprocessor_scaled),\n",
    "                                      ('classifier', SVC(gamma='auto', random_state=42))])\n",
    "\n",
    "pipeline_svm_scaled.fit(X_train, y_train)\n",
    "y_pred_svm_scaled = pipeline_svm_scaled.predict(X_test)\n",
    "accuracy_svm_scaled = accuracy_score(y_test, y_pred_svm_scaled)\n",
    "print(f\"Accuracy (SVM, Scaled Numerical Features): {accuracy_svm_scaled:.4f}\")\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(f\"- Decision Trees are generally less affected by feature scaling because they make decisions based on feature thresholds, not distances.\")\n",
    "print(f\"- SVMs, which use distance calculations (e.g., in the kernel), are highly sensitive to the scale of features. Scaling typically improves their performance.\")\n",
    "print(f\"- The results ({accuracy_dt_unscaled:.4f} vs {accuracy_dt_scaled:.4f} for DT; {accuracy_svm_unscaled} vs {accuracy_svm_scaled:.4f} for SVM) demonstrate this impact.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Question 10: Custom Transformations for Categorical Features (Frequency Encoding Example)\n",
    "print(\"Question 10: Custom Transformations for Categorical Features (Frequency Encoding Example)\")\n",
    "\n",
    "# Demonstrating a simple custom frequency encoding function\n",
    "# This is more useful for high cardinality features, but we'll apply it to 'Embarked' for demonstration\n",
    "\n",
    "def frequency_encode(df, column):\n",
    "    \"\"\"\n",
    "    Applies frequency encoding to a specified column in a DataFrame.\n",
    "    \"\"\"\n",
    "    # Calculate frequency of each category\n",
    "    freq_map = df[column].value_counts(normalize=True).to_dict()\n",
    "    # Map frequencies to the column\n",
    "    df[f'{column}_FreqEncoded'] = df[column].map(freq_map)\n",
    "    return df\n",
    "\n",
    "# Apply custom frequency encoding to 'Embarked'\n",
    "df_freq_encoded = frequency_encode(df.copy(), 'Embarked') # Use a copy to not modify the original df\n",
    "\n",
    "print(\"\\nDataFrame with 'Embarked' Frequency Encoded:\")\n",
    "print(df_freq_encoded[['Embarked', 'Embarked_FreqEncoded']].head())\n",
    "\n",
    "# Example of how it might look for a feature like 'Ticket' (if used, needs cleaning/handling)\n",
    "# print(\"\\nApplying Frequency Encoding to 'Ticket' (Example - requires handling non-numeric):\")\n",
    "# df_freq_encoded_ticket = frequency_encode(df.copy(), 'Ticket')\n",
    "# print(df_freq_encoded_ticket[['Ticket', 'Ticket_FreqEncoded']].head())\n",
    "# print(\"Note: Frequency encoding on 'Ticket' results in many unique values having low frequencies.\")\n",
    "\n",
    "\n",
    "print(\"\\nExplanation: Frequency encoding replaces each category with its frequency (or proportion) in the dataset. This can be useful for high-cardinality features by reducing the number of unique values to a single numerical feature.\")\n",
    "\n",
    "print(\"-\" * 30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
